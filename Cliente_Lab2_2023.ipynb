{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Francrodi/AprendAut_T2/blob/master/Cliente_Lab2_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "# Descarga las stopwords si aún no las tienes\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# delete date and time\n",
        "METADATA = r'^\\d+\\/\\d+\\/\\d+, \\d+:\\d+ - '\n",
        "AUTHOR = r'^[^:]+:\\s*'\n",
        "LAUGH = r'(?:[aA]*(?:[jJ][aA]*)+[jJ]?)\\b'\n",
        "\n",
        "def preprocess(path: str):\n",
        "    # Lee el archivo de conversaciones de WhatsApp\n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Elimina la fecha y hora de cada mensaje\n",
        "    text = re.sub(METADATA, '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Initialize an empty string to store the processed text\n",
        "    lines = text.splitlines()\n",
        "    text = \"\"\n",
        "    for line in lines:\n",
        "        # ignore lines with <Media omitted>\n",
        "        if \"<Media omitted>\" in line:\n",
        "            continue\n",
        "        # delete author and :\n",
        "        elif \":\" in line:\n",
        "            line = re.sub(AUTHOR, '', line)\n",
        "            text += line + \"\\n\"\n",
        "\n",
        "    # Elimina 3 letras repetidas, ej: 'hholaaa' -> 'hhola'\n",
        "    # no elimine dos por el tema de palabras con doble r, doble l, etc.\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "    # Elimina risas\n",
        "    text = re.sub(LAUGH, '', text)\n",
        "\n",
        "    # Tokenización: Divide el texto en palabras o términos\n",
        "    tokens = word_tokenize(text, language='spanish')\n",
        "\n",
        "    # Conversión a minúsculas y eliminación de signos de puntuación\n",
        "    clean_tokens = [token.lower() for token in tokens if re.match(r'^[a-zA-Z]+$', token)]\n",
        "\n",
        "    # Eliminación de stopwords (palabras comunes que generalmente no aportan información)\n",
        "    filtered_tokens = [token for token in clean_tokens if token not in set(stopwords.words('spanish'))]\n",
        "\n",
        "    # Ahora, 'filtered_tokens' contiene las palabras procesadas listas para ser utilizadas en tu modelo.\n",
        "    return filtered_tokens\n",
        "\n",
        "raw_vocabulary = preprocess('data/chat.txt')\n",
        "\n",
        "df = pd.DataFrame(raw_vocabulary, columns=['word'])\n",
        "# exclude words of 1 letter\n",
        "counts_df = df[df['word'].str.len() > 1]\n",
        "# make a new df ['word', 'occurrences']\n",
        "vocabulary = counts_df.groupby('word').size().reset_index(name='occurrences')\n",
        "\n",
        "mapeo = {\n",
        "    'toy': 'estoy',\n",
        "    'tamos': 'estamos',\n",
        "    'tamo': 'estamos',\n",
        "    'pal': 'para',\n",
        "    'vamo': 'vamos',\n",
        "    'tan': 'estan',\n",
        "    'tas': 'estas',\n",
        "}\n",
        "to_exclude = [ 'pa', 'bo', 'ta', 'va', ]\n",
        "\n",
        "voc2_df = vocabulary.copy()\n",
        "# replace words\n",
        "voc2_df['word'] = voc2_df['word'].replace(mapeo)\n",
        "# exclude words\n",
        "voc2_df = voc2_df[~voc2_df['word'].isin(to_exclude)]\n",
        "\n",
        "voc2_df.sort_values(by='occurrences', ascending=False).head(n=50)"
      ],
      "metadata": {
        "id": "6dcCj6_vS3Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "filename = \"chat.txt\"\n",
        "\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "  data = file.readlines()\n",
        "\n",
        "messages_list = []\n",
        "\n",
        "for line in data:\n",
        "  parts = line.split(\":\")\n",
        "  message = parts[-1].strip()\n",
        "  messages_list.append(message)\n",
        "\n",
        "def preprocess_chat(chat: list) -> list:\n",
        "  chat = [message.lower() for message in chat]\n",
        "  chat = [re.sub(r\"[^a-z\\s]\", \"\", message) for message in chat]\n",
        "  chat = [re.sub(r\"HASHTAG|URL|MENTION\", \"\", message) for message in chat]\n",
        "  chat = [message.strip() for message in chat]\n",
        "  chat = [message for message in chat if message != \"\"]\n",
        "  chat = [re.sub(r\"\\s+\", \" \", message) for message in chat]\n",
        "  chat = list(dict.fromkeys(chat))\n",
        "  chat.sort()\n",
        "  return chat\n",
        "\n",
        "messages_list = preprocess_chat(messages_list)\n",
        "\n",
        "messages_list"
      ],
      "metadata": {
        "id": "8bhZ-bQ5RHyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una lista con todas las palabras de los mensajes\n",
        "words_list = []\n",
        "\n",
        "for message in messages_list:\n",
        "  words = message.split(\" \")\n",
        "  for word in words:\n",
        "    words_list.append(word)\n",
        "\n",
        "# Crear un diccionario con las palabras y la cantidad de veces que aparecen\n",
        "words_dict = {}\n",
        "\n",
        "for word in words_list:\n",
        "  if word in words_dict:\n",
        "    words_dict[word] += 1\n",
        "  else:\n",
        "    words_dict[word] = 1\n",
        "\n",
        "words_dict"
      ],
      "metadata": {
        "id": "kqkgQTq0RJsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de palabras unicas (Vocabulario)\n",
        "vocabulary = list(dict.fromkeys(words_list))\n",
        "\n",
        "# Construimos diccionario de probabilidades de las palabras P(H)\n",
        "total_words = sum(words_dict.values())\n",
        "ph_dict = {word: frequency / total_words for word, frequency in words_dict.items()}\n",
        "\n",
        "# Construimos diccionario de probabilidades P(D|H) en base al hiperparametro N (Cantidad de palabras a considerar)\n",
        "# TODO: Revisar si se esta haciendo correctamente\n",
        "def build_pdh_dict(chat: list, N=2: int):\n",
        "  pdh_dict = {}\n",
        "  for message in chat:\n",
        "    words = message.split(\" \")\n",
        "    for i in range(len(words) - N + 1):\n",
        "      key = \" \".join(words[i:i+N-1])\n",
        "      value = words[i+N-1]\n",
        "      if key in pdh_dict:\n",
        "        if value in pdh_dict[key]:\n",
        "          pdh_dict[key][value] += 1\n",
        "        else:\n",
        "          pdh_dict[key][value] = 1\n",
        "      else:\n",
        "        pdh_dict[key] = {value: 1}\n",
        "\n",
        "  for key, value in pdh_dict.items():\n",
        "    total = sum(value.values())\n",
        "    pdh_dict[key] = {k: v / total for k, v in value.items()}\n",
        "\n",
        "  return pdh_dict\n",
        "\n",
        "pdh_dict = build_pdh_dict(messages_list, N=2)\n",
        "\n",
        "pdh_dict"
      ],
      "metadata": {
        "id": "IsOIB3cDOYSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Revisar, seguramente este mal\n",
        "def recomendacion_bayesiana(frase):\n",
        "  if len(frase) >= 2:\n",
        "    keys = frase[-2:]\n",
        "  else:\n",
        "    keys = frase\n",
        "\n",
        "  keys = [k.lower() for k in keys]\n",
        "\n",
        "  # Bucar la palabra con mayor probabilidad, es decir, que maximice P(H|D) = P(D|H) * P(H) (Hipoteses MAP)\n",
        "  max_prob = 0\n",
        "  palabra_sugerida = \"\"\n",
        "  for word in vocabulary:\n",
        "    prob = 0\n",
        "    for key in keys:\n",
        "      prob *= pdh_dict[word].get(key, 0.001)\n",
        "    prob *= ph_dict[word]\n",
        "    if prob >= max_prob:\n",
        "      max_prob = prob\n",
        "      palabra_sugerida = word\n",
        "\n",
        "  return palabra_sugerida\n",
        "\n",
        "def preprocesar_chat(chat):\n",
        "  # algo aca\n",
        "  pass\n",
        "\n",
        "def entrenar():\n",
        "  # Tener en cuenta el N\n",
        "  # Armar una estructura tipo diccionario de profundidad N\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "##### LOOP PRINCIPAL #####\n",
        "\n",
        "print(\"Ingrese la frase dando ENTER luego de \\x1b[3mcada palabra\\x1b[0m.\")\n",
        "print(\"Ingrese sólo ENTER para aceptar la recomendación sugerida, o escriba la siguiente palabra y de ENTER\")\n",
        "print(\"Ingrese '.' para comenzar con una frase nueva.\")\n",
        "print(\"Ingrese '..' para terminar el proceso.\")\n",
        "\n",
        "frase = []\n",
        "palabra_sugerida = \"\"\n",
        "while 1:\n",
        "    palabra = input(\">> \")\n",
        "\n",
        "    if palabra == \"..\":\n",
        "      break\n",
        "\n",
        "    elif palabra == \".\":\n",
        "      print(\"----- Comenzando frase nueva -----\")\n",
        "      frase = []\n",
        "\n",
        "    elif palabra == \"\": # acepta última palabra sugerida\n",
        "      frase.append(palabra_sugerida)\n",
        "\n",
        "    else: # escribió una palabra\n",
        "      frase.append(palabra)\n",
        "\n",
        "    if frase:\n",
        "      palabra_sugerida = recomendacion_bayesiana(frase)\n",
        "\n",
        "      frase_propuesta = frase.copy()\n",
        "      frase_propuesta.append(\"\\x1b[3m\"+ palabra_sugerida +\"\\x1b[0m\")\n",
        "\n",
        "      print(\" \".join(frase_propuesta))\n",
        "\n",
        "\n",
        "\n",
        "# hola chau buen dia PRED\n",
        "\n",
        "# p(buen|PRED)*p(dia|PRED)\n",
        "# p[PRED][\"dia\"]*p[PRED][\"buen\"]*p(PRED)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "VrHEaPPcj30i",
        "outputId": "8968570e-b7db-4891-dc6a-82aaf9ecf7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingrese la frase dando ENTER luego de \u001b[3mcada palabra\u001b[0m.\n",
            "Ingrese sólo ENTER para aceptar la recomendación sugerida, o escriba la siguiente palabra y de ENTER\n",
            "Ingrese '.' para comenzar con una frase nueva.\n",
            "Ingrese '..' para terminar el proceso.\n",
            ">> asdf\n",
            "asdf \u001b[3mMartes\u001b[0m\n",
            ">> \n",
            "asdf Martes \u001b[3mDomingo\u001b[0m\n",
            ">> .\n",
            "----- Comenzando frase nueva -----\n",
            ">> hola\n",
            "hola \u001b[3mViernes\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4f660c3ba775>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mpalabra_sugerida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpalabra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpalabra\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jbRhFQxc-MW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}